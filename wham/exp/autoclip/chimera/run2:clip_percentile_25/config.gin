# Macros:
# ==============================================================================
TRAIN_CACHE = '/media/sdg/cache/tr'
VAL_CACHE = '/media/sdg/cache/cv'
WHAM_ROOT = '/home/data/wham'

# Parameters for Adam:
# ==============================================================================
Adam.lr = 0.001

# Parameters for add_auto_balance_loss:
# ==============================================================================
add_auto_balance_loss.update_frequency = 100

# Parameters for add_autoclip_gradient_handler:
# ==============================================================================
add_autoclip_gradient_handler.clip_percentile = 25

# Parameters for add_train_handlers:
# ==============================================================================
add_train_handlers.handler_names = \
    ['add_lr_scheduler_handler',
     'add_autoclip_gradient_handler',
     'add_auto_balance_loss']

# Parameters for analyze:
# ==============================================================================
analyze.output_folder = @output_folder()

# Parameters for BSSEvalScale:
# ==============================================================================
BSSEvalScale.compute_permutation = True
BSSEvalScale.source_labels = ['s1', 's2']

# Parameters for test/build_dataset:
# ==============================================================================
test/build_dataset.dataset_class = @test/nussl.datasets.WHAM

# Parameters for train/build_dataset:
# ==============================================================================
train/build_dataset.dataset_class = @train/nussl.datasets.WHAM

# Parameters for val/build_dataset:
# ==============================================================================
val/build_dataset.dataset_class = @val/nussl.datasets.WHAM

# Parameters for build_model_optimizer_scheduler:
# ==============================================================================
build_model_optimizer_scheduler.model_config = \
    @nussl.ml.networks.builders.build_recurrent_chimera()
build_model_optimizer_scheduler.optimizer_class = @torch.optim.Adam
build_model_optimizer_scheduler.scheduler_class = \
    @torch.optim.lr_scheduler.ReduceLROnPlateau

# Parameters for build_recurrent_chimera:
# ==============================================================================
build_recurrent_chimera.bidirectional = True
build_recurrent_chimera.dropout = 0.3
build_recurrent_chimera.embedding_activation = ['sigmoid', 'unit_norm']
build_recurrent_chimera.embedding_size = 20
build_recurrent_chimera.hidden_size = 600
build_recurrent_chimera.mask_activation = ['sigmoid']
build_recurrent_chimera.num_features = 129
build_recurrent_chimera.num_layers = 4
build_recurrent_chimera.num_sources = 2

# Parameters for build_transforms:
# ==============================================================================
build_transforms.transform_names_and_args = \
    [('PhaseSensitiveSpectrumApproximation', {}),
     ('GetAudio', {}),
     ('MagnitudeWeights', {}),
     ('ToSeparationModel', {}),
     ('Cache', {}),
     ('GetExcerpt', {'excerpt_length': 400}),
     ('GetExcerpt',
      {'excerpt_length': 64000,
       'tf_keys': ['mix_audio', 'source_audio'],
       'time_dim': 1})]

# Parameters for train/build_transforms:
# ==============================================================================
train/build_transforms.cache_location = %TRAIN_CACHE

# Parameters for val/build_transforms:
# ==============================================================================
val/build_transforms.cache_location = %VAL_CACHE

# Parameters for cache:
# ==============================================================================
cache.batch_size = 40
cache.num_cache_workers = 40

# Parameters for DeepMaskEstimation:
# ==============================================================================
DeepMaskEstimation.model_path = @model_path()

# Parameters for evaluate:
# ==============================================================================
evaluate.block_on_gpu = True
evaluate.eval_class = @nussl.evaluation.BSSEvalScale
evaluate.num_workers = 1
evaluate.output_folder = @output_folder()
evaluate.seed = 0
evaluate.separation_algorithm = @nussl.separation.deep.DeepMaskEstimation

# Parameters for model_path:
# ==============================================================================
model_path.model_suffix = 'checkpoints/best.model.pth'

# Parameters for output_folder:
# ==============================================================================
output_folder._output_folder = \
    '/home/pseetharaman/Dropbox/research/nussl-models/wham/exp/autoclip/chimera/run2:clip_percentile_25'

# Parameters for ReduceLROnPlateau:
# ==============================================================================
ReduceLROnPlateau.factor = 0.5
ReduceLROnPlateau.patience = 5

# Parameters for STFTParams:
# ==============================================================================
STFTParams.hop_length = 64
STFTParams.window_length = 256
STFTParams.window_type = 'sqrt_hann'

# Parameters for sweep:
# ==============================================================================
sweep.parameters = \
    {'add_autoclip_gradient_handler.clip_percentile': [1, 10, 25, 50, 90, 100]}

# Parameters for train:
# ==============================================================================
train.batch_size = 25
train.device = 'cuda'
train.loss_dictionary = \
    {'PermutationInvariantLoss': {'args': ['L1Loss'], 'weight': 1},
     'WhitenedKMeansLoss': {'weight': 20000}}
train.num_data_workers = 1
train.num_epochs = 100
train.output_folder = @output_folder()
train.seed = 0
train.val_loss_dictionary = {'PermutationInvariantLoss': {'args': ['L1Loss']}}

# Parameters for stft_params/unginify:
# ==============================================================================
stft_params/unginify.kls = @nussl.STFTParams
stft_params/unginify.kls_name = 'nussl.STFTParams'

# Parameters for WHAM:
# ==============================================================================
WHAM.cache_populated = True
WHAM.mix_folder = 'mix_clean'
WHAM.mode = 'min'
WHAM.root = %WHAM_ROOT
WHAM.sample_rate = 8000
WHAM.stft_params = @stft_params/unginify()

# Parameters for test/WHAM:
# ==============================================================================
test/WHAM.split = 'tt'

# Parameters for train/WHAM:
# ==============================================================================
train/WHAM.split = 'tr'
train/WHAM.transform = @train/build_transforms()

# Parameters for val/WHAM:
# ==============================================================================
val/WHAM.split = 'cv'
val/WHAM.transform = @val/build_transforms()
